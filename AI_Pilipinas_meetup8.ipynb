{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Pilipinas Meet-Up 8 #\n",
    "This notebook contains notes on the AI Pilipinas Meetup held in The Globe Tower, Taguig City on May 22, 2019. The speakers for the said event were:\n",
    "\n",
    "1. **Julie Ann Salido PhD Student, De La Salle University**\n",
    "        Clinical Skin Cancer Detection for Asian Skin using Deep Convolutional Neural Networks\n",
    "2. **Ralph Vincent Regalado, CEO Senti AI**\n",
    "        On-Device Transfer Learning on the Coral Dev Board\n",
    "3. **Dr. Macario Cordel, II, Faculty, De La Salle University**\n",
    "        Emotion-aware Human Attention Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segment One: On-Device Transfer Learning on the Coral Dev Board ###\n",
    "\n",
    "1. ML in Production\n",
    "\n",
    "        A Challenge: computing power, cost. \n",
    "        Environment -> device -> internet connection -> ML server. \n",
    "2. ML at the Edge\n",
    "\n",
    "        Environment -> device.\n",
    "        Advantages: \n",
    "            offline inference. diretso device.\n",
    "            minimal latency. bye internet.\n",
    "            privacy and security. if user does not want to share data.\n",
    "3. What is Transfer Learning?\n",
    "        \n",
    "        Use knowledge gained while solving one problem and applying it to a different but related problem. \n",
    "        2 Sreps:\n",
    "            Pre Training.  \n",
    "            Fine Tuning. \n",
    "4. What is Coral?\n",
    "        \n",
    "        Platform for creating products with on devie ML acceleration.\n",
    "        Our first products feature Google's Esge TPU in SBC and USB accessory forms.\n",
    "        In short, it is a board. \n",
    "        Kinda like Raspberyy Pi but meant for ML. Do inferencing within the board. Supports TensorFlow Lite.\n",
    "            Coral Accelerator. Smol! Plug into USB onto your device like a laptop. Deploy model into USB device. Has a camera tool. \n",
    "5. Edge TPU\n",
    "           \n",
    "        Fast-ish?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segment Two: Clinical Skin Cancer Detection for Asian Skin using Deep Convolutional Neural Networks ###\n",
    "1. Background of the Study\n",
    "        \n",
    "        Skin is the biggest organ in our body. Epidermis, Dermins, Hypodermis.\n",
    "        Skin cancer begins in the epidermis.\n",
    "        Anyone can get skin cancer. Every hour, there is 3 skin cancer deaths. \n",
    "        Half of it is in Asia. \n",
    "        PH high mortality rate for skin cancer. \n",
    "        Most common in Chinese Asian, Malay, and Hispanics.\n",
    "        Sun damage is cumulative.\n",
    "        Basta problema sa Pinas \n",
    "2. Skin lesion segmentation and classification\n",
    "        \n",
    "        Segmenation is distinguishing background, foreground, from lesion.\n",
    "3. Research focus\n",
    "       \n",
    "       Design a classifier for skin cancer detection specifically for the Asian population.\n",
    "       Collect clinical and dermascopic images -> build benchmark images for training, validation, and testing clinical images that are prevalently photographs\n",
    "       -> Data preparation: test images and training images. \n",
    "       Test images should be biopsy proven. 5-8cm away from the lesion. Visible skin features. Should be taken outside or lighted room. Preprocessing: variations in skin tone, skin artifacts, non uniform lighting, non uniform vignetting, physical location of the lesion, variations in the lesion itself in terms of color, texture, shape, size, etc. Hair artifacts removal: morphological operators: median filter, bottomhat filter, binary image, area opening, area restoration (harmonic inpainting). All this doen in 5 lines of code. Crowd sourced. Perform skin lesion boundary segmentation. 96% ish accuracy on nonmelanoma and melanoma.\n",
    "       Augmentation techniques: image translation, image rotate, lesion border crop, image enhancement, \n",
    "       -> Design the CNN model. \n",
    "           Voting scheme. \n",
    "           Train, test, and validate classifier model for skin cancer classification of clinical images. \n",
    "           Train from scratch (end to end), clinical images\n",
    "               2 classes: benign VS malignant\n",
    "                   Testing images (77, 84)\n",
    "                   Trainimg images (3457)\n",
    "               6 + 3 hidden layers, ILR = 0.0001\n",
    "                   90.5 % accuracy.......\n",
    "                   Bilis niya magpalit ng slides\n",
    "           Already for clinical testing for skin cancer experts. \n",
    "julie_salido@dlsu.edu.ph\n",
    "jasalido@asu.edu.ph\n",
    "       \n",
    "how to balance classes? Data augmentation. What this?       \n",
    "Optimize model? 1GB to 10MB UwU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segment Three: Emotion-Aware Human Attention Prediction ###\n",
    "Predict where humans will fixate on in an image. \n",
    "1. Example applications:\n",
    "\n",
    "        teach autonomous car to look at traffic signs and stuff. \n",
    "        Use it for targetting as well (social advertising). \n",
    "        Robot vision. Mimick how a human would look given a certain task. \n",
    "2. Biological attention mechanism\n",
    "\n",
    "        Bottom up mechanism: \n",
    "            stimulus driven, \n",
    "            exogenous, \n",
    "            aturomatic, \n",
    "            rapid transient time course.\n",
    "        Top down mechanism\n",
    "            goal driven\n",
    "            endogenous\n",
    "            voluntary\n",
    "            slow time course\n",
    "            ex. seeing a friend in a crowd of faces\n",
    "3. Definition of terms\n",
    "        \n",
    "        Fixation map vs saliency map vs fixation location\n",
    "            fixation location - a binary (discreet-valued) image showing the location of eye fixation in an image. \n",
    "            fixation map - a gray scale value (continuos-valued) image derived from the fixation location by passing a gaussian filter, and ideally would be computed by taking an average over infinite subjects.\n",
    "            saliency map - prediction of the fixation map by an algorithm. probability distrivution.\n",
    "4. Current State of Technology\n",
    "        \n",
    "        Traditional Approaches\n",
    "            Features are registered automatically and in parallel across visual field. \n",
    "        Deep Learning - based approaches\n",
    "            Something about probability and linear combination\n",
    "            Human attention at different resolution is assembled\n",
    "            Some incorporate human central fixation bias by superimposing\n",
    "        Some website. Compares performance of different approaches (saliency model).\n",
    "        Challenge: best computing saleincy computational models have marginal performance over other saliency models. Best match ground truth.\n",
    "        Proposal: discover higher level concepts in images and reason out of the relative importance of image regions. \n",
    "        Ground Truth - look into this. \n",
    "5. Human Study on Attention and Emotion\n",
    "        \n",
    "        Emotional Dataset (EMOd) consists of 1019 emotional images.\n",
    "            with object level contour and sentiment labels\n",
    "            with eye fixation data from 16 human subjects\n",
    "        Initial attempts on incorporating emotion to human attention prediction. The most similar is [9] (???)\n",
    "            authors used \"object attention score\" (AS) to measure the object's attention level. \n",
    "            Consider consensus of human fixation (ex. scattered fixation points)\n",
    "        Solution to AS problem is to use HCS (consensus of human fixation)\n",
    "        For images contaiing objects with different sentiments: emotion evoking objects receive more attention. \n",
    "        modulated by image complexity. \n",
    "        objects in an image, regardless of whether emotion evoking or emotionally neutral objects will compete for attention\n",
    "6. The model\n",
    "        \n",
    "        Training and testing\n",
    "            caffe framework\n",
    "            semantic feature branchL finetuned using SALICON dataset using a bunch of hyperparameters\n",
    "            Sentiment mask branch \n",
    "        Trained using GeForce GTX Titan X\n",
    "        Evaluation Methods\n",
    "            evaluated 2 datasets containing emotional images\n",
    "                EMOd and CAT2000\n",
    "            Compared with commonly used models for benchmarking. \n",
    "         emotion-evoking objects appearing with neutral objects have higher increase in saliency levels VS when objects have similar sentiment. \n",
    "         Ground truth -> EASal without emotion information -> EASal with emotion information\n",
    "         \n",
    "*Emod is publicly available.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
